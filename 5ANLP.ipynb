{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyON25PbP8uDPt2DXJMCNhnl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fiG51l11NfMJ","executionInfo":{"status":"ok","timestamp":1763389776779,"user_tz":-60,"elapsed":16700,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"f629484e-a205-4085-d568-aaa7364c3b5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","    'I love my dog',\n","    'I love my cat',\n","    'You love my dog!',\n","]\n","\n","tokenizer = Tokenizer(num_words=100)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)"]},{"cell_type":"code","source":["text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=len(word_index) + 1,\n","                                                    standardize='lower_and_strip_punctuation' ,\n","                                                    split='whitespace' ,\n","                                                    ngrams=None ,\n","                                                    output_mode='int' ,\n","                                                    output_sequence_length=None ,\n","                                                    pad_to_max_tokens=True)\n","\n"],"metadata":{"id":"pdSUlUYzQMVh","executionInfo":{"status":"ok","timestamp":1763390563151,"user_tz":-60,"elapsed":3,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Le Padding (probleme de taille )\n"," ## Uniformiser la longueur"],"metadata":{"id":"rYzsPxtFS2VG"}},{"cell_type":"code","source":["text_vectorizer.adapt(sentences)\n"],"metadata":{"id":"G0VOGazES2CN","executionInfo":{"status":"ok","timestamp":1763391317930,"user_tz":-60,"elapsed":26,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Avant"],"metadata":{"id":"U3zV_b7BUpqF"}},{"cell_type":"code","source":["from numpy import vectorize\n","vectorized_text = text_vectorizer(sentences)\n","print(vectorized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXAunCN5RwxC","executionInfo":{"status":"ok","timestamp":1763391347798,"user_tz":-60,"elapsed":85,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"5b59cc1b-fa13-446b-f1a9-bcbfc9f97709"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[4 3 2 5]\n"," [4 3 2 1]\n"," [6 3 2 5]], shape=(3, 4), dtype=int64)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences,maxlen=5)\n","print(\"Index\" , word_index)\n","print(\"Sequence\" , sequences)\n","print(\"Padded\" , padded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOOnAhAXUL8a","executionInfo":{"status":"ok","timestamp":1763391488229,"user_tz":-60,"elapsed":37,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"a978da5c-19f1-4649-87b0-78a28641184c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Index {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n","Sequence [[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4]]\n","Padded [[0 3 1 2 4]\n"," [0 3 1 2 5]\n"," [0 6 1 2 4]]\n"]}]},{"cell_type":"code","source":["tokenizer = Tokenizer(num_words=100,oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences,maxlen=5)\n","print(\"Index\" , word_index)\n","print(\"Sequence\" , sequences)\n","print(\"Padded\" , padded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pgFRk4kXwq7","executionInfo":{"status":"ok","timestamp":1763392322061,"user_tz":-60,"elapsed":43,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"3c5ea4cd-2626-42bc-a957-e2f63d83919f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n","Index {'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n","Sequence [[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n","Padded [[0 4 2 3 5]\n"," [0 4 2 3 6]\n"," [0 7 2 3 5]]\n"]}]},{"cell_type":"markdown","source":["# Exercice"],"metadata":{"id":"w8lzgfraVw0Y"}},{"cell_type":"markdown","source":["1-Data"],"metadata":{"id":"bJUwnMFvWE4z"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Création de la base de données\n","data = {\n","    'phrase': [\n","\n","        \"Le chat dort paisiblement sur le canapé.\",\n","        \"J'aime beaucoup lire des livres le soir.\",\n","        \"Elle va au marché tous les dimanches matin.\",\n","        \"Les enfants jouent dans le jardin avec leurs amis.\",\n","        \"Mon frère travaille comme ingénieur informatique.\",\n","        \"Nous avons visité Paris pendant les vacances d'été.\",\n","        \"Le professeur explique la leçon de mathématiques.\",\n","        \"Il fait beau aujourd'hui, je vais me promener.\",\n","        \"Ma mère prépare un délicieux gâteau au chocolat.\",\n","        \"Les oiseaux chantent dans les arbres du parc.\",\n","\n","\n","        \"Le chat dort paisible sur le canapé.\",  # erreur d'accord\n","        \"J'aime beaucoup lire des livre le soir.\",  # pluriel manquant\n","        \"Elle va au marché tous les dimanche matin.\",  # pluriel manquant\n","        \"Les enfant jouent dans le jardin avec leurs amis.\",  # pluriel manquant\n","        \"Mon frère travaille comme ingénieur informatiques.\",  # pluriel incorrect\n","        \"Nous avons visité Paris pendant les vacance d'été.\",  # pluriel manquant\n","        \"Le professeur explique la leçons de mathématiques.\",  # accord incorrect\n","        \"Il fait beaux aujourd'hui, je vais me promener.\",  # accord incorrect\n","        \"Ma mère prépare un délicieux gâteaux au chocolat.\",  # pluriel incorrect\n","        \"Les oiseaux chante dans les arbres du parc.\",  # conjugaison incorrecte\n","    ],\n","    'label': [\n","        # Labels : 1 = bonne phrase, 0 = mauvaise phrase\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # bonnes phrases\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0   # mauvaises phrases\n","    ]\n","}\n","\n","# Créer un DataFrame\n","df = pd.DataFrame(data)\n","\n"],"metadata":{"id":"1oaJ8yqjWuPK","executionInfo":{"status":"ok","timestamp":1763392331057,"user_tz":-60,"elapsed":39,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["print(f\"\\nNombre total de phrases : {len(df)}\")\n","print(f\"Bonnes phrases : {sum(df['label'] == 1)}\")\n","print(f\"Mauvaises phrases : {sum(df['label'] == 0)}\")\n","\n","print(df.to_string(index=True))\n","\n","# Sauvegarder dans un fichier CSV\n","df.to_csv('phrases_dataset.csv', index=False, encoding='utf-8')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkYv1jifXX5Q","executionInfo":{"status":"ok","timestamp":1763392334245,"user_tz":-60,"elapsed":44,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"cb5f1032-b521-41ce-845d-fe58dd113a71"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Nombre total de phrases : 20\n","Bonnes phrases : 10\n","Mauvaises phrases : 10\n","                                                 phrase  label\n","0              Le chat dort paisiblement sur le canapé.      1\n","1              J'aime beaucoup lire des livres le soir.      1\n","2           Elle va au marché tous les dimanches matin.      1\n","3    Les enfants jouent dans le jardin avec leurs amis.      1\n","4     Mon frère travaille comme ingénieur informatique.      1\n","5   Nous avons visité Paris pendant les vacances d'été.      1\n","6     Le professeur explique la leçon de mathématiques.      1\n","7        Il fait beau aujourd'hui, je vais me promener.      1\n","8      Ma mère prépare un délicieux gâteau au chocolat.      1\n","9         Les oiseaux chantent dans les arbres du parc.      1\n","10                 Le chat dort paisible sur le canapé.      0\n","11              J'aime beaucoup lire des livre le soir.      0\n","12           Elle va au marché tous les dimanche matin.      0\n","13    Les enfant jouent dans le jardin avec leurs amis.      0\n","14   Mon frère travaille comme ingénieur informatiques.      0\n","15   Nous avons visité Paris pendant les vacance d'été.      0\n","16   Le professeur explique la leçons de mathématiques.      0\n","17      Il fait beaux aujourd'hui, je vais me promener.      0\n","18    Ma mère prépare un délicieux gâteaux au chocolat.      0\n","19          Les oiseaux chante dans les arbres du parc.      0\n"]}]},{"cell_type":"markdown","source":["2- tokenization"],"metadata":{"id":"3SCCmm2tYqFt"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"979974ff","executionInfo":{"status":"ok","timestamp":1763392569819,"user_tz":-60,"elapsed":15,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"b51ed561-afab-44fb-959f-993e90074a71"},"source":["phrases = df['phrase']\n","print(phrases.head())"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["0             Le chat dort paisiblement sur le canapé.\n","1             J'aime beaucoup lire des livres le soir.\n","2          Elle va au marché tous les dimanches matin.\n","3    Les enfants jouent dans le jardin avec leurs a...\n","4    Mon frère travaille comme ingénieur informatique.\n","Name: phrase, dtype: object\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d579c62","executionInfo":{"status":"ok","timestamp":1763392600551,"user_tz":-60,"elapsed":28,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"4638e193-8062-4d4f-8cc2-7893232ac347"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer(num_words=None, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(phrases)\n","word_index = tokenizer.word_index\n","print(\"Word Index:\", word_index)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Index: {'<OOV>': 1, 'le': 2, 'les': 3, 'au': 4, 'dans': 5, 'chat': 6, 'dort': 7, 'sur': 8, 'canapé': 9, \"j'aime\": 10, 'beaucoup': 11, 'lire': 12, 'des': 13, 'soir': 14, 'elle': 15, 'va': 16, 'marché': 17, 'tous': 18, 'matin': 19, 'jouent': 20, 'jardin': 21, 'avec': 22, 'leurs': 23, 'amis': 24, 'mon': 25, 'frère': 26, 'travaille': 27, 'comme': 28, 'ingénieur': 29, 'nous': 30, 'avons': 31, 'visité': 32, 'paris': 33, 'pendant': 34, \"d'été\": 35, 'professeur': 36, 'explique': 37, 'la': 38, 'de': 39, 'mathématiques': 40, 'il': 41, 'fait': 42, \"aujourd'hui\": 43, 'je': 44, 'vais': 45, 'me': 46, 'promener': 47, 'ma': 48, 'mère': 49, 'prépare': 50, 'un': 51, 'délicieux': 52, 'chocolat': 53, 'oiseaux': 54, 'arbres': 55, 'du': 56, 'parc': 57, 'paisiblement': 58, 'livres': 59, 'dimanches': 60, 'enfants': 61, 'informatique': 62, 'vacances': 63, 'leçon': 64, 'beau': 65, 'gâteau': 66, 'chantent': 67, 'paisible': 68, 'livre': 69, 'dimanche': 70, 'enfant': 71, 'informatiques': 72, 'vacance': 73, 'leçons': 74, 'beaux': 75, 'gâteaux': 76, 'chante': 77}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d42ead7d","executionInfo":{"status":"ok","timestamp":1763392634879,"user_tz":-60,"elapsed":34,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"36e859a9-fbb3-4a28-e4d7-c18fd68707a2"},"source":["sequences = tokenizer.texts_to_sequences(phrases)\n","print(sequences[:5])"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 6, 7, 58, 8, 2, 9], [10, 11, 12, 13, 59, 2, 14], [15, 16, 4, 17, 18, 3, 60, 19], [3, 61, 20, 5, 2, 21, 22, 23, 24], [25, 26, 27, 28, 29, 62]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f40dceeb","executionInfo":{"status":"ok","timestamp":1763392668848,"user_tz":-60,"elapsed":20,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"729fd032-59f0-43bb-fe70-5384f4e524c3"},"source":["max_sequence_length = max(len(seq) for seq in sequences)\n","print(f\"Maximum sequence length: {max_sequence_length}\")"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 9\n"]}]},{"cell_type":"markdown","source":["3- padding"],"metadata":{"id":"xIyM3qWQZZd4"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7515d64b","executionInfo":{"status":"ok","timestamp":1763392694611,"user_tz":-60,"elapsed":41,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"51de146d-8970-4e35-bdae-fbdd5e4d6ce0"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n","print(f\"Shape of padded sequences: {padded_sequences.shape}\")"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of padded sequences: (20, 9)\n"]}]},{"cell_type":"code","metadata":{"id":"f657a92f","executionInfo":{"status":"ok","timestamp":1763392843606,"user_tz":-60,"elapsed":1939,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}}},"source":["vocab_size = len(tokenizer.word_index) + 1\n","embedding_dim = 16\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(padded_sequences, labels, epochs=20, batch_size=32, verbose=0)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93cb0230","executionInfo":{"status":"ok","timestamp":1763392882388,"user_tz":-60,"elapsed":27,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"faae5aa6-e76e-4cec-b765-816bbf1b0645"},"source":["print(history.history)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["{'accuracy': [0.6000000238418579, 0.6000000238418579, 0.550000011920929, 0.6000000238418579, 0.550000011920929, 0.6000000238418579, 0.6499999761581421, 0.75, 0.75, 0.75, 0.800000011920929, 0.8500000238418579, 0.8500000238418579, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8500000238418579], 'loss': [0.6927465200424194, 0.6923956871032715, 0.692112147808075, 0.6918554306030273, 0.6916095018386841, 0.6913792490959167, 0.6911590695381165, 0.6909317970275879, 0.690711498260498, 0.6904870867729187, 0.6902562379837036, 0.6900290250778198, 0.6897972822189331, 0.6895555853843689, 0.6893094778060913, 0.6890453100204468, 0.6887764930725098, 0.6884939670562744, 0.6882012486457825, 0.6879022717475891]}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6b6463e","executionInfo":{"status":"ok","timestamp":1763392964525,"user_tz":-60,"elapsed":368,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"40de3048-a1a2-4be6-e614-5ae62f00137e"},"source":["loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","y_pred = (y_pred > 0.5).astype(int) # Convert probabilities to binary predictions\n","\n","print(\"\\nPredicted labels on test set:\", y_pred.flatten())\n","print(\"Actual labels on test set:\", y_test)\n"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.6861\n","Test Accuracy: 0.7500\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","\n","Predicted labels on test set: [1 1 0 1]\n","Actual labels on test set: [1 0 0 1]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2ed10e9","executionInfo":{"status":"ok","timestamp":1763392920379,"user_tz":-60,"elapsed":47,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"524c485d-4e25-464b-bf6b-50ea1962a9ea"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n","\n","print(f\"Training sequences shape: {X_train.shape}\")\n","print(f\"Testing sequences shape: {X_test.shape}\")\n","print(f\"Training labels shape: {y_train.shape}\")\n","print(f\"Testing labels shape: {y_test.shape}\")"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Training sequences shape: (16, 9)\n","Testing sequences shape: (4, 9)\n","Training labels shape: (16,)\n","Testing labels shape: (4,)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EGZHZf2TivG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"464f4d4c","executionInfo":{"status":"ok","timestamp":1763395156480,"user_tz":-60,"elapsed":344,"user":{"displayName":"Sabah Leml","userId":"17923721036110127669"}},"outputId":"43707c76-6868-4ec7-e430-15da789d2e95"},"source":["import numpy as np\n","\n","def classify_phrase(phrase):\n","    # Tokenize the new phrase\n","    sequence = tokenizer.texts_to_sequences([phrase])\n","\n","    # Pad the sequence to the same maxlen as the training data\n","    padded_sequence = pad_sequences(sequence, maxlen=maxlen)\n","\n","    # Make prediction\n","    prediction = model.predict(padded_sequence)\n","\n","    # Convert probability to binary label\n","    sentiment = (prediction > 0.5).astype(int)[0][0]\n","\n","    if sentiment == 1:\n","        return \"Good phrase\", prediction[0][0]\n","    else:\n","        return \"Bad phrase\", prediction[0][0]\n","\n","# Example usage:\n","phrase_to_test = \"Bonjour Monsieur !\"\n","result, probability = classify_phrase(phrase_to_test)\n","print(f\"Phrase: \\\"{phrase_to_test}\\\"\")\n","print(f\"Classification: {result} (Probability: {probability:.4f})\")\n","\n","phrase_to_test_2 = \"Je n'aime pas ce chat .\"\n","result_2, probability_2 = classify_phrase(phrase_to_test_2)\n","print(f\"\\nPhrase: \\\"{phrase_to_test_2}\\\"\")\n","print(f\"Classification: {result_2} (Probability: {probability_2:.4f})\")\n","\n","phrase_to_test_3 = \"Bonjoure Monsieur ! \"\n","result_3, probability_3 = classify_phrase(phrase_to_test_3)\n","print(f\"\\nPhrase: \\\"{phrase_to_test_3}\\\"\")\n","print(f\"Classification: {result_3} (Probability: {probability_3:.4f})\")\n"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n","Phrase: \"Bonjour Monsieur !\"\n","Classification: Good phrase (Probability: 0.5019)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","\n","Phrase: \"Je n'aime pas ce chat .\"\n","Classification: Bad phrase (Probability: 0.4996)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","\n","Phrase: \"Bonjoure Monsieur ! \"\n","Classification: Good phrase (Probability: 0.5019)\n"]}]},{"cell_type":"code","metadata":{"id":"8860729a"},"source":["vocab_size = len(tokenizer.word_index) + 1\n","embedding_dim = 16\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dropout(0.5), # Added Dropout layer\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# The model needs to be re-trained after architectural changes. This line will be re-executed.\n","history = model.fit(padded_sequences, labels, epochs=20, batch_size=32, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"215b2997"},"source":["# Task\n","Confirm that the Keras model architecture has been updated with the `tf.keras.layers.Dropout(0.5)` layer."]},{"cell_type":"markdown","metadata":{"id":"de712aee"},"source":["## Final Task\n","\n","### Subtask:\n","Confirm that the model architecture has been successfully modified and is ready for re-training by inspecting the model summary.\n"]},{"cell_type":"markdown","metadata":{"id":"63c37bb3"},"source":["## Summary:\n","\n","### Q&A\n","*   **Was the Keras model architecture updated with the `tf.keras.layers.Dropout(0.5)` layer?**\n","    *   The provided solving process did not include steps or output to confirm whether the `tf.keras.layers.Dropout(0.5)` layer was successfully added to the model architecture.\n","\n","### Data Analysis Key Findings\n","*   No analysis was performed in the provided solving process to confirm the presence of the `tf.keras.layers.Dropout(0.5)` layer in the model architecture.\n","\n","### Insights or Next Steps\n","*   To confirm the model architecture update, inspect the `model.summary()` output and verify the presence of the `Dropout` layer with a rate of 0.5.\n","*   Once confirmed, the model will be ready for re-training with the modified architecture.\n"]}]}